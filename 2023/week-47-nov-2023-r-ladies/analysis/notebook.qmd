---
title: "TidyTuesday 2023-47 Analysis"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

# Tidytuesday 2023-47

This is the notebook for analysis of the data of week 47 (2023) for the [tidytuesday](https://tidytues.day) project. The game is on!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytuesdayR)
library(hrbrthemes)
library(scales)
library(lubridate)
library(ggrepel)
library(patchwork)
library(here)
library(gganimate)
library(patchwork)
library(camcorder)
library(ragg)
library(tidytext)
library(broom)
# Set theme
theme_set(theme_ipsum() + theme(legend.position = "bottom"))

#Initialize Camcorder
gg_record(
  dir = file.path(getwd(), "camcorder"), 
  device = "png", 
  dpi = 300    
)

# Define default path for saving plots
plot_path <- here::here("output", paste0("tidytuesday_", 2023, "_", 47))
if (!dir.exists(plot_path)) {
  dir.create(plot_path, recursive = TRUE)
}

```

Okay let us see what we have.

```{r}
# Load TidyTuesday Data
tuesdata <- tidytuesdayR::tt_load(2023, week = 47)$rladies_chapters


names(tuesdata)
glimpse(tuesdata[[1]])

```

## Questions

### Can we extract locations?

```{r}
# This is the easy way BUT not the suggested one.
locations <- tuesdata %>%
  separate(chapter, into = c("chapter", "place"), sep = "-", extra = "merge") %>% 
  select(-id, chapter) %>% 
  geocode(place, method = "osm") %>%
  rename(latitude = lat, longitude = long)

# How many did we not get and what are they?
locations %>% filter(is.na(latitude)) %>% 
  group_by(place, chapter) %>% 
  summarise(count = n())

# ldnont is London, Ontario and rladiesnyc was not separated by -
# No worries, we can add it in manually
locations <- locations %>%
  filter(is.na(latitude)) %>%
  select(-latitude, -longitude) %>% 
  mutate(
    place = case_when(
      place == "ldnont" ~ "London, Ontario",
      chapter == "rladiesnyc" ~ "New York City",
      TRUE ~ place
    )
  ) %>%
  geocode(place, method = "osm") %>% 
  rename(latitude = lat, longitude = long) %>% 
  bind_rows(locations) %>% 
  drop_na()
```

This is *okay* but not perfect and prone to errors. For example, `rladies-tc` refers to Twin Cities and trusting Nominatim to geocode that just based on `tc` is optimistic. Instead, we can use the fact that the Meetup.com links for each chapter specify the location if it is applicable. We can scrape that into a new column and THEN run geocoding on it.

```{r}
library(rvest)
library(future)
library(furrr)

# Set up parallel processing
plan(multisession)

# Function to extract location text with error handling
extract_location <- function(chapter) {
  url <- paste0("https://www.meetup.com/", chapter)
  
  # Try to read the HTML, if it fails, return NA
  tryCatch({
    page <- read_html(url)
    location_text <-
      page %>% html_node("#city-link div.text-sm") %>% html_text()
    return(location_text)
  }, error = function(e) {
    message("Error with chapter: ", chapter, ". Error: ", e$message)
    return(NA)
  })
}

# Extract distinct chapters
distinct_chapters <- distinct(tuesdata, chapter)

cities <- tibble(
  chapter = distinct_chapters$chapter,
  chapter_city = future_map_chr(distinct_chapters$chapter, extract_location)
)

# What didn't make it?
cities %>%
  filter(is.na(chapter_city))

cities <- cities %>%
  mutate(chapter_city = if_else(
    is.na(chapter_city),
    case_when(
      chapter == "rladies-toronto" ~ "Toronto",
      chapter == "rladies-wageningen" ~ "Wageningen, Netherlands",
      chapter == "rladies-loughborough" ~ "Loughborough, UK",
      chapter == "rladies-rabat" ~ "Rabat, Morocco",
      chapter == "rladies-cincinnati" ~ "Cincinnati, Ohio",
      chapter == "rladies-la-plata" ~ "La Plata, Argentina",
      chapter == "rladies-tiruchirappalli" ~ "Tiruchirappalli",
      chapter == "rladies-leuven" ~ "Leuven, Belgium",
      chapter == "rladies-louisville" ~ "Louisville, Kentucky"
    ),
    chapter_city
  ))
```

*Now* let us geocode this to perfection.

```{r}
library(tidygeocoder)

geocoded <- cities %>%
  mutate(chapter_city = str_replace(chapter_city, ",\\s*\\d+$", "")) %>%
  geocode(chapter_city, method = "osm") %>%
  rename(latitude = lat, longitude = long)

# And now join it into the data

data <- tuesdata %>% 
  left_join(., geocoded, by = "chapter")
```

### Can we use word embeddings to classify titles or find similar topics?

```{r}
library(httr)
library(jsonlite)

# Function to split the titles into chunks
split_into_chunks <- function(titles, chunk_size) {
  split(titles, ceiling(seq_along(titles) / chunk_size))
}

# Split data$title into chunks 
chunk_size <- 1000
title_chunks <- split_into_chunks(data$title, chunk_size)

# Function to get embeddings for a chunk
get_embeddings_for_chunk <- function(chunk) {
  embeddings_url <- "https://api.openai.com/v1/embeddings"
  auth <-
    add_headers(Authorization = paste("Bearer", Sys.getenv("OPENAI_API_KEY")))
  body <- list(model = "text-embedding-ada-002", input = chunk)
  
  resp <- POST(embeddings_url, auth, body = body, encode = "json")
  embeddings <- content(resp, as = "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE) %>%
    pluck("data", "embedding")
  
  return(embeddings)
}

# Iterate over chunks and collect embeddings
all_embeddings <- lapply(title_chunks, get_embeddings_for_chunk)
embeddings <- do.call(c, all_embeddings)

title_embeddings <-
  data %>%
  mutate(embeddings = embeddings)

embeddings_mat <- matrix(unlist(title_embeddings$embeddings),
                         ncol = 1536,
                         byrow = TRUE)

embeddings_similarity <-
  embeddings_mat / sqrt(rowSums(embeddings_mat * embeddings_mat))
embeddings_similarity <-
  embeddings_similarity %*% t(embeddings_similarity)

# enframe(embeddings_similarity[88, ], name = "title", value = "similarity") %>%
#   arrange(-similarity)
# 
# data %>%
#   slice(c(433, 2438, 985, 2436)) %>%
#   select(title, chapter)
```

We can keep viewing these one by one but what I'd like to do is collapse them into a single list for each chapter.

```{r}
similarity_threshold <- 0.9

get_similar_indices <- function(row_index, similarity_matrix) {
  similarities <- similarity_matrix[row_index, ]
  indices <- seq_along(similarities)
  
  tibble(indices, similarities) %>%
    filter(similarities >= similarity_threshold, indices != row_index) %>%
    arrange(desc(similarities)) %>%
    with(setNames(similarities, indices))
}

df_similarity <- data %>%
  ungroup() %>% 
  mutate(eventId = row_number()) %>%
  mutate(similarEvents = map(eventId, ~ get_similar_indices(.x, embeddings_similarity))) %>% group_by(chapter) %>%
  mutate(total_events = n()) %>% 
  ungroup()

```

## Using OpenAI API to return keywords

```{r}
library(httr)
library(jsonlite)

collapsed_data <- data %>%
  group_by(chapter) %>%
  summarize(titles = paste(title, collapse = ", ")) %>%
  ungroup()

promptText <- "Review the provided list of event titles related to R programming and its community. Extract and list up to 15 unique, generalized keywords. Each keyword should be no more than three words long and represent broader themes, tools, or practices common in the R community. Avoid including specific dates, individual names, or unique events. Keywords should not be specific to a single event but should reflect recurring concepts or tools in the R community. Standardize terms for consistency and focus on categorizing the type of concept each keyword represents (e.g., programming tool, data analysis technique, community activity)."

get_keywords <- safely(function(titles, progress) {
  api_key <- Sys.getenv("OPENAI_API_KEY")
  embeddings_url <- "https://api.openai.com/v1/chat/completions"
  message_list <- list(
    list(role = "system", content = "You are a helpful assistant designed to output JSON."),
    list(
      role = "user",
      content = paste(promptText, titles)
    )
  )
  
  body <- list(
    model = "gpt-3.5-turbo-1106",
    response_format = list(type = "json_object"),
    messages = message_list
  )
  
  response <- POST(
    url = embeddings_url,
    add_headers(
      `Content-Type` = "application/json",
      Authorization = paste("Bearer", api_key)
    ),
    body = body,
    encode = "json"
  )
  
  response_content <- content(response, as = "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE)
  
  json_string <- response_content$choices$message.content[1]

   if (jsonlite::validate(json_string)) {
    parsed_json <- fromJSON(json_string)
    if (!is.null(parsed_json$keywords)) {
      keywords_list <- parsed_json$keywords
      keywords_string <- paste(keywords_list, collapse = ", ")
    } else {
      keywords_string <- NA
    }
  } else {
    keywords_string <- NA
  }
  progress$tick()$print()
  return(keywords_string)
}, otherwise = NA_character_)


progress_bar <- progress_estimated(length(collapsed_data$titles))

keyword_data <- collapsed_data %>%
  mutate(keywords = map(titles, ~get_keywords(.x, progress_bar)))

keywords <- keyword_data %>% 
  unnest_longer(keywords) %>% 
  filter(keywords_id != "error") %>%  
  select(-titles) %>% 
  separate_rows(keywords, sep = ",\\s*") %>% 
  mutate(keywords = str_to_lower(keywords)) %>%
  filter(!keywords %in% c('r programming', 'data analysis', 'r-ladies community', 'r-ladies')) %>% 
  group_by(chapter) %>%
  summarize(keywords = paste(unique(keywords), collapse = ", ")) %>%
  ungroup()


  
```

## Finding out which r-ladies chapters are closely alike

```{r}

chapter_similarity <- df_similarity %>%
  unnest_longer(col = similarEvents) %>%
  rename(similarity_score = similarEvents, similar_eventId = similarEvents_id) %>%
  mutate(
    chapter_id = as.integer(factor(chapter)),
    similar_chapter = map_chr(similar_eventId, ~ df_similarity$chapter[df_similarity$eventId == .x]),
    similar_chapter_id = as.integer(factor(similar_chapter))
  ) %>%
  group_by(chapter_id, similar_chapter_id, chapter) %>%
  summarize(mean_similarity_score = mean(similarity_score, na.rm = TRUE)) %>%
  ungroup() %>%
  group_by(chapter_id, chapter) %>%
  top_n(5, mean_similarity_score) %>%
  ungroup() %>% 
group_by(chapter_id, chapter) %>%
  summarize(similar_chapters = toString(similar_chapter_id[similar_chapter_id != chapter_id]))

```

## Output Files

We want to export this

```{r}

# Combine everything into one 
combined <- data %>% 
  ungroup() %>% 
  left_join(chapter_similarity, by = "chapter") %>%
  left_join(keywords, by = 'chapter') %>% 
  mutate(similar_chapters = str_split(similar_chapters, ",\\s*") %>%
  map(~as.numeric(.x))) %>% 
  select(
    chapter_id, 
    chapter, 
    chapter_city, 
    total_events,
    location, 
    latitude,
    longitude,
    similar_chapters,
    keywords,
    title,
    date
  )

json_data <- combined %>%
  group_by(
    chapter_id,
    chapter,
    chapter_city,
    total_events,
    latitude,
    longitude,
    similar_chapters,
    keywords,
  ) %>%
  nest(meetups = c(title, location, date)) 

jsonlite::write_json(json_data, "data/output.json")
```
